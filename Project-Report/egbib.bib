
@article{applyDQN,
  author    = {Melrose Roderick and
               James MacGlashan and
               Stefanie Tellex},
  title     = {Implementing the Deep Q-Network},
  journal   = {CoRR},
  volume    = {abs/1711.07478},
  year      = {2017},
  url       = {http://arxiv.org/abs/1711.07478},
  archivePrefix = {arXiv},
  eprint    = {1711.07478},
  timestamp = {Mon, 13 Aug 2018 16:48:02 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1711-07478},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@book{Sutton1988ReinforcementLA,
  title={Reinforcement Learning: An Introduction},
  author={Richard S. Sutton and Andrew G. Barto},
  year={1988},
  volume={16},
  pages={285-286}
}

@article{double_qlearning,
  author    = {Hado van Hasselt and
               Arthur Guez and
               David Silver},
  title     = {Deep Reinforcement Learning with Double Q-learning},
  journal   = {CoRR},
  volume    = {abs/1509.06461},
  year      = {2015},
  url       = {http://arxiv.org/abs/1509.06461},
  archivePrefix = {arXiv},
  eprint    = {1509.06461},
  timestamp = {Mon, 13 Aug 2018 16:47:32 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/HasseltGS15},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{IMPALA,
  author    = {Lasse Espeholt and
               Hubert Soyer and
               R{\'{e}}mi Munos and
               Karen Simonyan and
               Volodymyr Mnih and
               Tom Ward and
               Yotam Doron and
               Vlad Firoiu and
               Tim Harley and
               Iain Dunning and
               Shane Legg and
               Koray Kavukcuoglu},
  title     = {{IMPALA:} Scalable Distributed Deep-RL with Importance Weighted Actor-Learner
               Architectures},
  journal   = {CoRR},
  volume    = {abs/1802.01561},
  year      = {2018},
  url       = {http://arxiv.org/abs/1802.01561},
  archivePrefix = {arXiv},
  eprint    = {1802.01561},
  timestamp = {Mon, 13 Aug 2018 16:46:52 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1802-01561},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{cobbe_quantifying_2018,
	title = {Quantifying {Generalization} in {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1812.02341},
	abstract = {In this paper, we investigate the problem of overfitting in deep reinforcement learning. Among the most common benchmarks in RL, it is customary to use the same environments for both training and testing. This practice offers relatively little insight into an agent's ability to generalize. We address this issue by using procedurally generated environments to construct distinct training and test sets. Most notably, we introduce a new environment called CoinRun, designed as a benchmark for generalization in RL. Using CoinRun, we find that agents overfit to surprisingly large training sets. We then show that deeper convolutional architectures improve generalization, as do methods traditionally found in supervised learning, including L2 regularization, dropout, data augmentation and batch normalization.},
	urldate = {2019-02-27},
	journal = {arXiv:1812.02341 [cs, stat]},
	author = {Cobbe, Karl and Klimov, Oleg and Hesse, Chris and Kim, Taehoon and Schulman, John},
	month = dec,
	year = {2018},
	note = {arXiv: 1812.02341},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning}
}

@article{bengio_representation_2012,
	title = {Representation {Learning}: {A} {Review} and {New} {Perspectives}},
	shorttitle = {Representation {Learning}},
	url = {https://arxiv.org/abs/1206.5538v3},
	abstract = {The success of machine learning algorithms generally depends on data
representation, and we hypothesize that this is because different
representations can entangle and hide more or less the different explanatory
factors of variation behind the data. Although specific domain knowledge can be
used to help design representations, learning with generic priors can also be
used, and the quest for AI is motivating the design of more powerful
representation-learning algorithms implementing such priors. This paper reviews
recent work in the area of unsupervised feature learning and deep learning,
covering advances in probabilistic models, auto-encoders, manifold learning,
and deep networks. This motivates longer-term unanswered questions about the
appropriate objectives for learning good representations, for computing
representations (i.e., inference), and the geometrical connections between
representation learning, density estimation and manifold learning.},
	language = {en},
	urldate = {2019-02-18},
	author = {Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
	month = jun,
	year = {2012}
}

@article{sparse_reward,
  author    = {Martin A. Riedmiller and
               Roland Hafner and
               Thomas Lampe and
               Michael Neunert and
               Jonas Degrave and
               Tom Van de Wiele and
               Volodymyr Mnih and
               Nicolas Heess and
               Jost Tobias Springenberg},
  title     = {Learning by Playing - Solving Sparse Reward Tasks from Scratch},
  journal   = {CoRR},
  volume    = {abs/1802.10567},
  year      = {2018},
  url       = {http://arxiv.org/abs/1802.10567},
  archivePrefix = {arXiv},
  eprint    = {1802.10567},
  timestamp = {Mon, 13 Aug 2018 16:47:06 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1802-10567},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{silver_mastering_2016,
	title = {Mastering the game of {Go} with deep neural networks and tree search},
	volume = {529},
	issn = {0028-0836, 1476-4687},
	url = {http://www.nature.com/articles/nature16961},
	doi = {10.1038/nature16961},
	language = {en},
	number = {7587},
	urldate = {2019-02-18},
	journal = {Nature},
	author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and van den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
	month = jan,
	year = {2016},
	pages = {484--489}
}


@article{silver_mastering_2017,
	title = {Mastering the game of {Go} without human knowledge},
	volume = {550},
	issn = {0028-0836, 1476-4687},
	url = {http://www.nature.com/doifinder/10.1038/nature24270},
	doi = {10.1038/nature24270},
	language = {en},
	number = {7676},
	urldate = {2019-02-26},
	journal = {Nature},
	author = {Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and Chen, Yutian and Lillicrap, Timothy and Hui, Fan and Sifre, Laurent and van den Driessche, George and Graepel, Thore and Hassabis, Demis},
	month = oct,
	year = {2017},
	pages = {354--359}
}


@article{pascanu_learning_2017,
	title = {Learning model-based planning from scratch},
	url = {http://arxiv.org/abs/1707.06170},
	abstract = {Conventional wisdom holds that model-based planning is a powerful approach to sequential decision-making. It is often very challenging in practice, however, because while a model can be used to evaluate a plan, it does not prescribe how to construct a plan. Here we introduce the "Imagination-based Planner", the first model-based, sequential decision-making agent that can learn to construct, evaluate, and execute plans. Before any action, it can perform a variable number of imagination steps, which involve proposing an imagined action and evaluating it with its model-based imagination. All imagined actions and outcomes are aggregated, iteratively, into a "plan context" which conditions future real and imagined actions. The agent can even decide how to imagine: testing out alternative imagined actions, chaining sequences of actions together, or building a more complex "imagination tree" by navigating flexibly among the previously imagined states using a learned policy. And our agent can learn to plan economically, jointly optimizing for external rewards and computational costs associated with using its imagination. We show that our architecture can learn to solve a challenging continuous control problem, and also learn elaborate planning strategies in a discrete maze-solving task. Our work opens a new direction toward learning the components of a model-based planning system and how to use them.},
	urldate = {2019-02-24},
	journal = {arXiv:1707.06170 [cs, stat]},
	author = {Pascanu, Razvan and Li, Yujia and Vinyals, Oriol and Heess, Nicolas and Buesing, Lars and Racanière, Sebastien and Reichert, David and Weber, Théophane and Wierstra, Daan and Battaglia, Peter},
	month = jul,
	year = {2017},
	note = {arXiv: 1707.06170},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning}
}


@article{mnih_playing_2013,
	title = {Playing {Atari} with {Deep} {Reinforcement} {Learning}},
	url = {https://arxiv.org/abs/1312.5602v1},
	abstract = {We present the first deep learning model to successfully learn control
policies directly from high-dimensional sensory input using reinforcement
learning. The model is a convolutional neural network, trained with a variant
of Q-learning, whose input is raw pixels and whose output is a value function
estimating future rewards. We apply our method to seven Atari 2600 games from
the Arcade Learning Environment, with no adjustment of the architecture or
learning algorithm. We find that it outperforms all previous approaches on six
of the games and surpasses a human expert on three of them.},
	language = {en},
	urldate = {2019-02-18},
	author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
	month = dec,
	year = {2013}
}

@article{mnih_human-level_2015,
	title = {Human-level control through deep reinforcement learning},
	volume = {518},
	issn = {0028-0836, 1476-4687},
	url = {http://www.nature.com/articles/nature14236},
	doi = {10.1038/nature14236},
	language = {en},
	number = {7540},
	urldate = {2019-02-18},
	journal = {Nature},
	author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
	month = feb,
	year = {2015},
	pages = {529--533}
}


@article{lample_playing_2016,
	title = {Playing {FPS} {Games} with {Deep} {Reinforcement} {Learning}},
	url = {https://arxiv.org/abs/1609.05521v2},
	abstract = {Advances in deep reinforcement learning have allowed autonomous agents to
perform well on Atari games, often outperforming humans, using only raw pixels
to make their decisions. However, most of these games take place in 2D
environments that are fully observable to the agent. In this paper, we present
the first architecture to tackle 3D environments in first-person shooter games,
that involve partially observable states. Typically, deep reinforcement
learning methods only utilize visual input for training. We present a method to
augment these models to exploit game feature information such as the presence
of enemies or items, during the training phase. Our model is trained to
simultaneously learn these features along with minimizing a Q-learning
objective, which is shown to dramatically improve the training speed and
performance of our agent. Our architecture is also modularized to allow
different models to be independently trained for different phases of the game.
We show that the proposed architecture substantially outperforms built-in AI
agents of the game as well as humans in deathmatch scenarios.},
	language = {en},
	urldate = {2019-02-18},
	author = {Lample, Guillaume and Chaplot, Devendra Singh},
	month = sep,
	year = {2016}
}

@incollection{littman_markov_1994,
	title = {Markov games as a framework for multi-agent reinforcement learning},
	isbn = {978-1-55860-335-6},
	url = {https://linkinghub.elsevier.com/retrieve/pii/B9781558603356500271},
	abstract = {In the Markov decision process (MDP) formalization of reinforcement learning, a single adaptive agent interacts with an environment deﬁned by a probabilistic transition function. In this solipsistic view, secondary agents can only be part of the environment and are therefore ﬁxed in their behavior. The framework of Markov games allows us to widen this view to include multiple adaptive agents with interacting or competing goals. This paper considers a step in this direction in which exactly two agents with diametrically opposed goals share an environment. It describes a Q-learning-like algorithm for ﬁnding optimal policies and demonstrates its application to a simple two-player game in which the optimal policy is probabilistic.},
	language = {en},
	urldate = {2019-02-14},
	booktitle = {Machine {Learning} {Proceedings} 1994},
	publisher = {Elsevier},
	author = {Littman, Michael L.},
	year = {1994},
	doi = {10.1016/B978-1-55860-335-6.50027-1},
	pages = {157--163}
}


@inproceedings{Ouderaa2016DeepRL,
  title={Deep Reinforcement Learning in Pac-man},
  author={Tycho van der Ouderaa and Efstratios Gavves and Matthias Reisser},
  year={2016}
}

@article{tesauro_temporal_nodate,
	title = {Temporal {Difference} {Learning} and {TD}-{Gammon}},
	language = {en},
	author = {Tesauro, Gerald},
	pages = {16},
	year = {1992}
}


@inproceedings{matignon_coordinated_2012,
	title = {Coordinated {Multi}-{Robot} {Exploration} {Under} {Communication} {Constraints} {Using} {Decentralized} {Markov} {Decision} {Processes}},
	copyright = {Authors who publish a paper in this conference agree to the following terms:   Author(s) agree to transfer their copyrights in their article/paper to the Association for the Advancement of Artificial Intelligence (AAAI), in order to deal with future requests for reprints, translations, anthologies, reproductions, excerpts, and other publications. This grant will include, without limitation, the entire copyright in the article/paper in all countries of the world, including all renewals, extensions, and reversions thereof, whether such rights current exist or hereafter come into effect, and also the exclusive right to create electronic versions of the article/paper, to the extent that such right is not subsumed under copyright.  The author(s) warrants that they are the sole author and owner of the copyright in the above article/paper, except for those portions shown to be in quotations; that the article/paper is original throughout; and that the undersigned right to make the grants set forth above is complete and unencumbered.  The author(s) agree that if anyone brings any claim or action alleging facts that, if true, constitute a breach of any of the foregoing warranties, the author(s) will hold harmless and indemnify AAAI, their grantees, their licensees, and their distributors against any liability, whether under judgment, decree, or compromise, and any legal fees and expenses arising out of that claim or actions, and the undersigned will cooperate fully in any defense AAAI may make to such claim or action. Moreover, the undersigned agrees to cooperate in any claim or other action seeking to protect or enforce any right the undersigned has granted to AAAI in the article/paper. If any such claim or action fails because of facts that constitute a breach of any of the foregoing warranties, the undersigned agrees to reimburse whomever brings such claim or action for expenses and attorneys’ fees incurred therein.  Author(s) retain all proprietary rights other than copyright (such as patent rights).  Author(s) may make personal reuse of all or portions of the above article/paper in other works of their own authorship.  Author(s) may reproduce, or have reproduced, their article/paper for the author’s personal use, or for company use provided that AAAI copyright and the source are indicated, and that the copies are not used in a way that implies AAAI endorsement of a product or service of an employer, and that the copies per se are not offered for sale. The foregoing right shall not permit the posting of the article/paper in electronic or digital form on any computer network, except by the author or the author’s employer, and then only on the author’s or the employer’s own web page or ftp site. Such web page or ftp site, in addition to the aforementioned requirements of this Paragraph, must provide an electronic reference or link back to the AAAI electronic server, and shall not post other AAAI copyrighted materials not of the author’s or the employer’s creation (including tables of contents with links to other papers) without AAAI’s written permission.  Author(s) may make limited distribution of all or portions of their article/paper prior to publication.  In the case of work performed under U.S. Government contract, AAAI grants the U.S. Government royalty-free permission to reproduce all or portions of the above article/paper, and to authorize others to do so, for U.S. Government purposes.  In the event the above article/paper is not accepted and published by AAAI, or is withdrawn by the author(s) before acceptance by AAAI, this agreement becomes null and void.},
	url = {https://www.aaai.org/ocs/index.php/AAAI/AAAI12/paper/view/5038},
	abstract = {Recent works on multi-agent sequential decision making using decentralized partially observable Markov decision processes have been concerned with interaction-oriented resolution techniques and provide promising results. These techniques take advantage of local interactions and coordination. In this paper, we propose an approach based on an interaction-oriented resolution of decentralized decision makers. To this end, distributed value functions (DVF) have been used by decoupling the multi-agent problem into a set of individual agent problems. However existing DVF techniques assume permanent and free communication between the agents. In this paper, we extend the DVF methodology to address full local observability, limited share of information and communication breaks. We apply our new DVF in a real-world application consisting of multi-robot exploration where each robot computes locally a strategy that minimizes the interactions between the robots and maximizes the space coverage of the team even under communication constraints. Our technique has been implemented and evaluated in simulation and in real-world scenarios during a robotic challenge for the exploration and mapping of an unknown environment. Experimental results from real-world scenarios and from the challenge are given where our system was vice-champion.},
	language = {en},
	urldate = {2019-02-24},
	booktitle = {Twenty-{Sixth} {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Matignon, Laetitia and Jeanpierre, Laurent and Mouaddib, Abdel-Illah},
	month = jul,
	year = {2012}
}


@article{leibo_multi-agent_2017,
	title = {Multi-agent {Reinforcement} {Learning} in {Sequential} {Social} {Dilemmas}},
	url = {https://arxiv.org/abs/1702.03037v1},
	abstract = {Matrix games like Prisoner's Dilemma have guided research on social dilemmas
for decades. However, they necessarily treat the choice to cooperate or defect
as an atomic action. In real-world social dilemmas these choices are temporally
extended. Cooperativeness is a property that applies to policies, not
elementary actions. We introduce sequential social dilemmas that share the
mixed incentive structure of matrix game social dilemmas but also require
agents to learn policies that implement their strategic intentions. We analyze
the dynamics of policies learned by multiple self-interested independent
learning agents, each using its own deep Q-network, on two Markov games we
introduce here: 1. a fruit Gathering game and 2. a Wolfpack hunting game. We
characterize how learned behavior in each domain changes as a function of
environmental factors including resource abundance. Our experiments show how
conflict can emerge from competition over shared resources and shed light on
how the sequential nature of real world social dilemmas affects cooperation.},
	language = {en},
	urldate = {2019-02-24},
	author = {Leibo, Joel Z. and Zambaldi, Vinicius and Lanctot, Marc and Marecki, Janusz and Graepel, Thore},
	month = feb,
	year = {2017}
}


@article{agogino_multiagent_2012,
	title = {A multiagent approach to managing air traffic flow},
	volume = {24},
	issn = {1387-2532, 1573-7454},
	url = {https://link-springer-com.ezproxy.library.uvic.ca/article/10.1007/s10458-010-9142-5},
	doi = {10.1007/s10458-010-9142-5},
	abstract = {Intelligent air traffic flow management is one of the fundamental challenges facing the Federal Aviation Administration (FAA) today. FAA estimates put weather, routing decisions and airport condition...},
	language = {en},
	number = {1},
	urldate = {2019-02-24},
	journal = {Autonomous Agents and Multi-Agent Systems},
	author = {Agogino, Adrian K. and Tumer, Kagan},
	month = jan,
	year = {2012},
	pages = {1--25}
}


@inproceedings{pipattanasomporn_multi-agent_2009,
	title = {Multi-agent systems in a distributed smart grid: {Design} and implementation},
	shorttitle = {Multi-agent systems in a distributed smart grid},
	doi = {10.1109/PSCE.2009.4840087},
	abstract = {The objective of this paper is to discuss the design and implementation of a multi-agent system that provides intelligence to a distributed smart grid - a smart grid located at a distribution level. A multi-agent application development will be discussed that involves agent specification, application analysis, application design and application realization. The message exchange in the proposed multi-agent system is designed to be compatible with an IP-based network (IP = Internet Protocol) which is based on the IEEE standard on Foundation for Intelligent Physical Agent (FIPA). The paper demonstrates the use of multi-agent systems to control a distributed smart grid in a simulated environment. The simulation results indicate that the proposed multi-agent system can facilitate the seamless transition from grid connected to an island mode when upstream outages are detected. This denotes the capability of a multi-agent system as a technology for managing the microgrid operation.},
	booktitle = {2009 {IEEE}/{PES} {Power} {Systems} {Conference} and {Exposition}},
	author = {Pipattanasomporn, M. and Feroze, H. and Rahman, S.},
	month = mar,
	year = {2009},
	keywords = {agent specification, Control system synthesis, Distributed control, distributed power generation, distributed smart grid, Distributed smart grid, Intelligent agent, Intelligent networks, intelligent physical agent, Intelligent systems, IP networks, IP-based network, microgrid operation, multi-agent system and microgrid, multi-agent systems, Multiagent systems, power engineering computing, Protocols, Smart grids, Technology management},
	pages = {1--8}
}

@article{busoniu_comprehensive_2008,
	title = {A {Comprehensive} {Survey} of {Multiagent} {Reinforcement} {Learning}},
	volume = {38},
	issn = {1094-6977, 1558-2442},
	url = {http://ieeexplore.ieee.org/document/4445757/},
	doi = {10.1109/TSMCC.2007.913919},
	abstract = {Multiagent systems are rapidly ﬁnding applications in a variety of domains, including robotics, distributed control, telecommunications, and economics. The complexity of many tasks arising in these domains makes them difﬁcult to solve with preprogrammed agent behaviors. The agents must, instead, discover a solution on their own, using learning. A signiﬁcant part of the research on multiagent learning concerns reinforcement learning techniques. This paper provides a comprehensive survey of multiagent reinforcement learning (MARL). A central issue in the ﬁeld is the formal statement of the multiagent learning goal. Different viewpoints on this issue have led to the proposal of many different goals, among which two focal points can be distinguished: stability of the agents’ learning dynamics, and adaptation to the changing behavior of the other agents. The MARL algorithms described in the literature aim—either explicitly or implicitly—at one of these two goals or at a combination of both, in a fully cooperative, fully competitive, or more general setting. A representative selection of these algorithms is discussed in detail in this paper, together with the speciﬁc issues that arise in each category. Additionally, the beneﬁts and challenges of MARL are described along with some of the problem domains where the MARL techniques have been applied. Finally, an outlook for the ﬁeld is provided.},
	language = {en},
	number = {2},
	urldate = {2019-02-14},
	journal = {IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)},
	author = {Busoniu, L. and Babuska, R. and De Schutter, B.},
	month = mar,
	year = {2008},
	pages = {156--172}
}


@article{vinyals_starcraft_2017,
	title = {{StarCraft} {II}: {A} {New} {Challenge} for {Reinforcement} {Learning}},
	shorttitle = {{StarCraft} {II}},
	url = {https://arxiv.org/abs/1708.04782v1},
	abstract = {This paper introduces SC2LE (StarCraft II Learning Environment), a
reinforcement learning environment based on the StarCraft II game. This domain
poses a new grand challenge for reinforcement learning, representing a more
difficult class of problems than considered in most prior work. It is a
multi-agent problem with multiple players interacting; there is imperfect
information due to a partially observed map; it has a large action space
involving the selection and control of hundreds of units; it has a large state
space that must be observed solely from raw input feature planes; and it has
delayed credit assignment requiring long-term strategies over thousands of
steps. We describe the observation, action, and reward specification for the
StarCraft II domain and provide an open source Python-based interface for
communicating with the game engine. In addition to the main game maps, we
provide a suite of mini-games focusing on different elements of StarCraft II
gameplay. For the main game maps, we also provide an accompanying dataset of
game replay data from human expert players. We give initial baseline results
for neural networks trained from this data to predict game outcomes and player
actions. Finally, we present initial baseline results for canonical deep
reinforcement learning agents applied to the StarCraft II domain. On the
mini-games, these agents learn to achieve a level of play that is comparable to
a novice player. However, when trained on the main game, these agents are
unable to make significant progress. Thus, SC2LE offers a new and challenging
environment for exploring deep reinforcement learning algorithms and
architectures.},
	language = {en},
	urldate = {2019-02-24},
	author = {Vinyals, Oriol and Ewalds, Timo and Bartunov, Sergey and Georgiev, Petko and Vezhnevets, Alexander Sasha and Yeo, Michelle and Makhzani, Alireza and Küttler, Heinrich and Agapiou, John and Schrittwieser, Julian and Quan, John and Gaffney, Stephen and Petersen, Stig and Simonyan, Karen and Schaul, Tom and van Hasselt, Hado and Silver, David and Lillicrap, Timothy and Calderone, Kevin and Keet, Paul and Brunasso, Anthony and Lawrence, David and Ekermo, Anders and Repp, Jacob and Tsing, Rodney},
	month = aug,
	year = {2017}
}

@article{arulkumaran_alphastar:_2019,
	title = {{AlphaStar}: {An} {Evolutionary} {Computation} {Perspective}},
	shorttitle = {{AlphaStar}},
	url = {https://arxiv.org/abs/1902.01724v2},
	abstract = {In January 2019, DeepMind revealed AlphaStar to the world-the first
artificial intelligence (AI) system to beat a professional player at the game
of StarCraft II-representing a milestone in the progress of AI. AlphaStar draws
on many areas of AI research, including deep learning, reinforcement learning,
game theory, and evolutionary computation (EC). In this paper we analyze
AlphaStar primarily through the lens of EC, presenting a new look at the system
and relating it to many concepts in the field. We highlight some of its most
interesting aspects-the use of Lamarckian evolution, competitive co-evolution,
and quality diversity. In doing so, we hope to provide a bridge between the
wider EC community and one of the most significant AI systems developed in
recent times.},
	language = {en},
	urldate = {2019-02-24},
	author = {Arulkumaran, Kai and Cully, Antoine and Togelius, Julian},
	month = feb,
	year = {2019}
}


@article{francois-lavet_introduction_2018,
	title = {An {Introduction} to {Deep} {Reinforcement} {Learning}},
	volume = {11},
	issn = {1935-8237, 1935-8245},
	url = {http://arxiv.org/abs/1811.12560},
	doi = {10.1561/2200000071},
	abstract = {Deep reinforcement learning is the combination of reinforcement learning (RL) and deep learning. This ﬁeld of research has been able to solve a wide range of complex decisionmaking tasks that were previously out of reach for a machine. Thus, deep RL opens up many new applications in domains such as healthcare, robotics, smart grids, ﬁnance, and many more. This manuscript provides an introduction to deep reinforcement learning models, algorithms and techniques. Particular focus is on the aspects related to generalization and how deep RL can be used for practical applications. We assume the reader is familiar with basic machine learning concepts.},
	language = {en},
	number = {3-4},
	urldate = {2019-02-14},
	journal = {Foundations and Trends® in Machine Learning},
	author = {Francois-Lavet, Vincent and Henderson, Peter and Islam, Riashat and Bellemare, Marc G. and Pineau, Joelle},
	year = {2018},
	note = {arXiv: 1811.12560},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	pages = {219--354}
}

@article{lecun_deep_2015,
	title = {Deep learning},
	volume = {521},
	issn = {1476-4687},
	url = {http://ezproxy.library.uvic.ca/login?url=http://search.ebscohost.com/login.aspx?direct=true&db=mnh&AN=26017442&site=ehost-live&scope=site},
	doi = {10.1038/nature14539},
	abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.;},
	number = {7553},
	urldate = {2019-02-24},
	journal = {Nature},
	author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
	month = may,
	year = {2015},
	keywords = {Algorithms, Artificial Intelligence*/trends, Computers, Language, Neural Networks (Computer)},
	pages = {436--444}
}


@article{watkins_q-learning_1992,
	title = {Q-learning},
	volume = {8},
	issn = {1573-0565},
	url = {https://doi.org/10.1007/BF00992698},
	doi = {10.1007/BF00992698},
	abstract = {Q-learning (Watkins, 1989) is a simple way for agents to learn how to act optimally in controlled Markovian domains. It amounts to an incremental method for dynamic programming which imposes limited computational demands. It works by successively improving its evaluations of the quality of particular actions at particular states.This paper presents and proves in detail a convergence theorem forQ-learning based on that outlined in Watkins (1989). We show thatQ-learning converges to the optimum action-values with probability 1 so long as all actions are repeatedly sampled in all states and the action-values are represented discretely. We also sketch extensions to the cases of non-discounted, but absorbing, Markov environments, and where manyQ values can be changed each iteration, rather than just one.},
	language = {en},
	number = {3},
	urldate = {2019-02-24},
	journal = {Machine Learning},
	author = {Watkins, Christopher J. C. H. and Dayan, Peter},
	month = may,
	year = {1992},
	keywords = {asynchronous dynamic programming, Q-learning, reinforcement learning, temporal differences},
	pages = {279--292}
}


@article{cesa-bianchi_boltzmann_2017,
	title = {Boltzmann {Exploration} {Done} {Right}},
	url = {https://arxiv.org/abs/1705.10257v2},
	abstract = {Boltzmann exploration is a classic strategy for sequential decision-making
under uncertainty, and is one of the most standard tools in Reinforcement
Learning (RL). Despite its widespread use, there is virtually no theoretical
understanding about the limitations or the actual benefits of this exploration
scheme. Does it drive exploration in a meaningful way? Is it prone to
misidentifying the optimal actions or spending too much time exploring the
suboptimal ones? What is the right tuning for the learning rate? In this paper,
we address several of these questions in the classic setup of stochastic
multi-armed bandits. One of our main results is showing that the Boltzmann
exploration strategy with any monotone learning-rate sequence will induce
suboptimal behavior. As a remedy, we offer a simple non-monotone schedule that
guarantees near-optimal performance, albeit only when given prior access to key
problem parameters that are typically not available in practical situations
(like the time horizon \$T\$ and the suboptimality gap \$Δ\$). More
importantly, we propose a novel variant that uses different learning rates for
different arms, and achieves a distribution-dependent regret bound of order
\${\textbackslash}frac\{K{\textbackslash}log{\textasciicircum}2 T\}Δ\$ and a distribution-independent bound of order
\${\textbackslash}sqrt\{KT\}{\textbackslash}log K\$ without requiring such prior knowledge. To demonstrate the
flexibility of our technique, we also propose a variant that guarantees the
same performance bounds even if the rewards are heavy-tailed.},
	language = {en},
	urldate = {2019-02-24},
	author = {Cesa-Bianchi, Nicolò and Gentile, Claudio and Lugosi, Gábor and Neu, Gergely},
	month = may,
	year = {2017},
}


@article{gordon_stable_nodate,
	title = {Stable {Fitted} {Reinforcement} {Learning}},
	abstract = {We describe the reinforcement learning problem, motivate algorithms which seek an approximation to the Q function, and present new convergence results for two such algorithms.},
	language = {en},
	author = {Gordon, Geoffrey J},
	pages = {7}
}


@incollection{hasselt_double_2010,
	title = {Double {Q}-learning},
	url = {http://papers.nips.cc/paper/3964-double-q-learning.pdf},
	urldate = {2019-02-24},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 23},
	publisher = {Curran Associates, Inc.},
	author = {Hasselt, Hado V.},
	editor = {Lafferty, J. D. and Williams, C. K. I. and Shawe-Taylor, J. and Zemel, R. S. and Culotta, A.},
	year = {2010},
	pages = {2613--2621},
	file = {NIPS Full Text PDF:C\:\\Users\\sumee\\Zotero\\storage\\T9NL7TDK\\Hasselt - 2010 - Double Q-learning.pdf:application/pdf;NIPS Snapshot:C\:\\Users\\sumee\\Zotero\\storage\\JI394ILG\\3964-double-q-learning.html:text/html}
}


@article{wang_dueling_2015,
	title = {Dueling {Network} {Architectures} for {Deep} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1511.06581},
	abstract = {In recent years there have been many successes of using deep representations in reinforcement learning. Still, many of these applications use conventional architectures, such as convolutional networks, LSTMs, or auto-encoders. In this paper, we present a new neural network architecture for model-free reinforcement learning. Our dueling network represents two separate estimators: one for the state value function and one for the state-dependent action advantage function. The main benefit of this factoring is to generalize learning across actions without imposing any change to the underlying reinforcement learning algorithm. Our results show that this architecture leads to better policy evaluation in the presence of many similar-valued actions. Moreover, the dueling architecture enables our RL agent to outperform the state-of-the-art on the Atari 2600 domain.},
	urldate = {2019-02-24},
	journal = {arXiv:1511.06581 [cs]},
	author = {Wang, Ziyu and Schaul, Tom and Hessel, Matteo and van Hasselt, Hado and Lanctot, Marc and de Freitas, Nando},
	month = nov,
	year = {2015},
	note = {arXiv: 1511.06581},
	keywords = {Computer Science - Machine Learning},
	annote = {Comment: 15 pages, 5 figures, and 5 tables},
	file = {arXiv\:1511.06581 PDF:C\:\\Users\\sumee\\Zotero\\storage\\NP6UTWE8\\Wang et al. - 2015 - Dueling Network Architectures for Deep Reinforceme.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\sumee\\Zotero\\storage\\PASJMXJZ\\1511.html:text/html}
}


@article{morimura_nonparametric_nodate,
	title = {Nonparametric {Return} {Distribution} {Approximation}  for {Reinforcement} {Learning}},
	abstract = {Standard Reinforcement Learning (RL) aims to optimize decision-making rules in terms of the expected return. However, especially for risk-management purposes, other criteria such as the expected shortfall are sometimes preferred. Here, we describe a method of approximating the distribution of returns, which allows us to derive various kinds of information about the returns. We ﬁrst show that the Bellman equation, which is a recursive formula for the expected return, can be extended to the cumulative return distribution. Then we derive a nonparametric return distribution estimator with particle smoothing based on this extended Bellman equation. A key aspect of the proposed algorithm is to represent the recursion relation in the extended Bellman equation by a simple replacement procedure of particles associated with a state by using those of the successor state. We show that our algorithm leads to a risksensitive RL paradigm. The usefulness of the proposed approach is demonstrated through numerical experiments.},
	language = {en},
	author = {Morimura, Tetsuro and Sugiyama, Masashi and Kashima, Hisashi and Hachiya, Hirotaka and Tanaka, Toshiyuki},
	pages = {8}
}

@article{sutton_learning_1988,
	title = {Learning to predict by the methods of temporal differences},
	volume = {3},
	issn = {1573-0565},
	url = {https://doi.org/10.1007/BF00115009},
	doi = {10.1007/BF00115009},
	abstract = {This article introduces a class of incremental learning procedures specialized for prediction-that is, for using past experience with an incompletely known system to predict its future behavior. Whereas conventional prediction-learning methods assign credit by means of the difference between predicted and actual outcomes, the new methods assign credit by means of the difference between temporally successive predictions. Although such temporal-difference methods have been used in Samuel's checker player, Holland's bucket brigade, and the author's Adaptive Heuristic Critic, they have remained poorly understood. Here we prove their convergence and optimality for special cases and relate them to supervised-learning methods. For most real-world prediction problems, temporal-difference methods require less memory and less peak computation than conventional methods and they produce more accurate predictions. We argue that most problems to which supervised learning is currently applied are really prediction problems of the sort to which temporal-difference methods can be applied to advantage.},
	language = {en},
	number = {1},
	urldate = {2019-02-24},
	journal = {Machine Learning},
	author = {Sutton, Richard S.},
	month = aug,
	year = {1988},
	keywords = {connectionism, credit assignment, evaluation functions, Incremental learning, prediction},
	pages = {9--44}
}


@article{hessel_rainbow:_2017,
	title = {Rainbow: {Combining} {Improvements} in {Deep} {Reinforcement} {Learning}},
	shorttitle = {Rainbow},
	url = {http://arxiv.org/abs/1710.02298},
	abstract = {The deep reinforcement learning community has made several independent improvements to the DQN algorithm. However, it is unclear which of these extensions are complementary and can be fruitfully combined. This paper examines six extensions to the DQN algorithm and empirically studies their combination. Our experiments show that the combination provides state-of-the-art performance on the Atari 2600 benchmark, both in terms of data efficiency and final performance. We also provide results from a detailed ablation study that shows the contribution of each component to overall performance.},
	urldate = {2019-02-24},
	journal = {arXiv:1710.02298 [cs]},
	author = {Hessel, Matteo and Modayil, Joseph and van Hasselt, Hado and Schaul, Tom and Ostrovski, Georg and Dabney, Will and Horgan, Dan and Piot, Bilal and Azar, Mohammad and Silver, David},
	month = oct,
	year = {2017},
	note = {arXiv: 1710.02298},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	annote = {Comment: Under review as a conference paper at AAAI 2018}
}


@inproceedings{thomas_bias_2014,
	title = {Bias in {Natural} {Actor}-{Critic} {Algorithms}},
	url = {http://proceedings.mlr.press/v32/thomas14.html},
	abstract = {We show that several popular discounted reward natural actor-critics, including the popular NAC-LSTD and eNAC algorithms, do not generate unbiased estimates of the natural policy gradient as claime...},
	language = {en},
	urldate = {2019-02-24},
	booktitle = {International {Conference} on {Machine} {Learning}},
	author = {Thomas, Philip},
	month = jan,
	year = {2014},
	pages = {441--448}
}


@article{lillicrap_continuous_2015,
	title = {Continuous control with deep reinforcement learning},
	url = {http://arxiv.org/abs/1509.02971},
	abstract = {We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies end-to-end: directly from raw pixel inputs.},
	urldate = {2019-02-24},
	journal = {arXiv:1509.02971 [cs, stat]},
	author = {Lillicrap, Timothy P. and Hunt, Jonathan J. and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
	month = sep,
	year = {2015},
	note = {arXiv: 1509.02971},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: 10 pages + supplementary}
}


@incollection{hasselt_double_2010,
	title = {Double {Q}-learning},
	url = {http://papers.nips.cc/paper/3964-double-q-learning.pdf},
	urldate = {2019-02-24},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 23},
	publisher = {Curran Associates, Inc.},
	author = {Hasselt, Hado V.},
	editor = {Lafferty, J. D. and Williams, C. K. I. and Shawe-Taylor, J. and Zemel, R. S. and Culotta, A.},
	year = {2010},
	pages = {2613--2621}
}

@article{mnih_asynchronous_2016,
	title = {Asynchronous {Methods} for {Deep} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1602.01783},
	abstract = {We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.},
	urldate = {2019-02-24},
	journal = {arXiv:1602.01783 [cs]},
	author = {Mnih, Volodymyr and Badia, Adrià Puigdomènech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy P. and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
	month = feb,
	year = {2016},
	note = {arXiv: 1602.01783},
	keywords = {Computer Science - Machine Learning}
}

@article{konda_actor-critic_nodate,
	title = {Actor-{Critic} {Algorithms}},
	abstract = {We propose and analyze a class of actor-critic algorithms for simulation-based optimization of a Markov decision process over a parameterized family of randomized stationary policies. These are two-time-scale algorithms in which the critic uses TD learning with a linear approximation architecture and the actor is updated in an approximate gradient direction based on information provided by the critic. We show that the features for the critic should span a subspace prescribed by the choice of parameterization of the actor. We conclude by discussing convergence properties and some open problems.},
	language = {en},
	author = {Konda, Vijay R and Tsitsiklis, John N},
	pages = {7}
}

@article{wang_sample_2016,
	title = {Sample {Efficient} {Actor}-{Critic} with {Experience} {Replay}},
	url = {http://arxiv.org/abs/1611.01224},
	abstract = {This paper presents an actor-critic deep reinforcement learning agent with experience replay that is stable, sample efficient, and performs remarkably well on challenging environments, including the discrete 57-game Atari domain and several continuous control problems. To achieve this, the paper introduces several innovations, including truncated importance sampling with bias correction, stochastic dueling network architectures, and a new trust region policy optimization method.},
	urldate = {2019-02-24},
	journal = {arXiv:1611.01224 [cs]},
	author = {Wang, Ziyu and Bapst, Victor and Heess, Nicolas and Mnih, Volodymyr and Munos, Remi and Kavukcuoglu, Koray and de Freitas, Nando},
	month = nov,
	year = {2016},
	note = {arXiv: 1611.01224},
	keywords = {Computer Science - Machine Learning},
	annote = {Comment: 20 pages. Prepared for ICLR 2017}
}

@article{gruslys_reactor:_2017,
	title = {The {Reactor}: {A} fast and sample-efficient {Actor}-{Critic} agent for {Reinforcement} {Learning}},
	shorttitle = {The {Reactor}},
	url = {http://arxiv.org/abs/1704.04651},
	abstract = {In this work we present a new agent architecture, called Reactor, which combines multiple algorithmic and architectural contributions to produce an agent with higher sample-efficiency than Prioritized Dueling DQN (Wang et al., 2016) and Categorical DQN (Bellemare et al., 2017), while giving better run-time performance than A3C (Mnih et al., 2016). Our first contribution is a new policy evaluation algorithm called Distributional Retrace, which brings multi-step off-policy updates to the distributional reinforcement learning setting. The same approach can be used to convert several classes of multi-step policy evaluation algorithms designed for expected value evaluation into distributional ones. Next, we introduce the {\textbackslash}b\{eta\}-leave-one-out policy gradient algorithm which improves the trade-off between variance and bias by using action values as a baseline. Our final algorithmic contribution is a new prioritized replay algorithm for sequences, which exploits the temporal locality of neighboring observations for more efficient replay prioritization. Using the Atari 2600 benchmarks, we show that each of these innovations contribute to both the sample efficiency and final agent performance. Finally, we demonstrate that Reactor reaches state-of-the-art performance after 200 million frames and less than a day of training.},
	urldate = {2019-02-24},
	journal = {arXiv:1704.04651 [cs]},
	author = {Gruslys, Audrunas and Dabney, Will and Azar, Mohammad Gheshlaghi and Piot, Bilal and Bellemare, Marc and Munos, Remi},
	month = apr,
	year = {2017},
	note = {arXiv: 1704.04651},
	keywords = {Computer Science - Artificial Intelligence}
}

@article{kakade_natural_nodate,
	title = {A {Natural} {Policy} {Gradient}},
	abstract = {We provide a natural gradient method that represents the steepest descent direction based on the underlying structure of the parameter space. Although gradient methods cannot make large changes in the values of the parameters, we show that the natural gradient is moving toward choosing a greedy optimal action rather than just a better action. These greedy optimal actions are those that would be chosen under one improvement step of policy iteration with approximate, compatible value functions, as defined by Sutton et al. [9]. We then show drastic performance improvements in simple MDPs and in the more challenging MDP of Tetris.},
	language = {en},
	author = {Kakade, Sham M},
	pages = {8}
}

@article{schulman_trust_2015,
	title = {Trust {Region} {Policy} {Optimization}},
	url = {http://arxiv.org/abs/1502.05477},
	abstract = {We describe an iterative procedure for optimizing policies, with guaranteed monotonic improvement. By making several approximations to the theoretically-justified procedure, we develop a practical algorithm, called Trust Region Policy Optimization (TRPO). This algorithm is similar to natural policy gradient methods and is effective for optimizing large nonlinear policies such as neural networks. Our experiments demonstrate its robust performance on a wide variety of tasks: learning simulated robotic swimming, hopping, and walking gaits; and playing Atari games using images of the screen as input. Despite its approximations that deviate from the theory, TRPO tends to give monotonic improvement, with little tuning of hyperparameters.},
	urldate = {2019-02-24},
	journal = {arXiv:1502.05477 [cs]},
	author = {Schulman, John and Levine, Sergey and Moritz, Philipp and Jordan, Michael I. and Abbeel, Pieter},
	month = feb,
	year = {2015},
	note = {arXiv: 1502.05477},
	keywords = {Computer Science - Machine Learning},
	annote = {Comment: 16 pages, ICML 2015}
}

@article{schulman_proximal_2017,
	title = {Proximal {Policy} {Optimization} {Algorithms}},
	url = {http://arxiv.org/abs/1707.06347},
	abstract = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a "surrogate" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.},
	urldate = {2019-02-24},
	journal = {arXiv:1707.06347 [cs]},
	author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
	month = jul,
	year = {2017},
	note = {arXiv: 1707.06347},
	keywords = {Computer Science - Machine Learning}
}

@article{odonoghue_combining_2016,
	title = {Combining policy gradient and {Q}-learning},
	url = {http://arxiv.org/abs/1611.01626},
	abstract = {Policy gradient is an efficient technique for improving a policy in a reinforcement learning setting. However, vanilla online variants are on-policy only and not able to take advantage of off-policy data. In this paper we describe a new technique that combines policy gradient with off-policy Q-learning, drawing experience from a replay buffer. This is motivated by making a connection between the fixed points of the regularized policy gradient algorithm and the Q-values. This connection allows us to estimate the Q-values from the action preferences of the policy, to which we apply Q-learning updates. We refer to the new technique as 'PGQL', for policy gradient and Q-learning. We also establish an equivalency between action-value fitting techniques and actor-critic algorithms, showing that regularized policy gradient techniques can be interpreted as advantage function learning algorithms. We conclude with some numerical examples that demonstrate improved data efficiency and stability of PGQL. In particular, we tested PGQL on the full suite of Atari games and achieved performance exceeding that of both asynchronous advantage actor-critic (A3C) and Q-learning.},
	urldate = {2019-02-24},
	journal = {arXiv:1611.01626 [cs, math, stat]},
	author = {O'Donoghue, Brendan and Munos, Remi and Kavukcuoglu, Koray and Mnih, Volodymyr},
	month = nov,
	year = {2016},
	note = {arXiv: 1611.01626},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Mathematics - Optimization and Control, Statistics - Machine Learning}
}

@article{browne_survey_2012,
	title = {A {Survey} of {Monte} {Carlo} {Tree} {Search} {Methods}},
	volume = {4},
	issn = {1943-068X},
	doi = {10.1109/TCIAIG.2012.2186810},
	abstract = {Monte Carlo tree search (MCTS) is a recently proposed search method that combines the precision of tree search with the generality of random sampling. It has received considerable interest due to its spectacular success in the difficult problem of computer Go, but has also proved beneficial in a range of other domains. This paper is a survey of the literature to date, intended to provide a snapshot of the state of the art after the first five years of MCTS research. We outline the core algorithm's derivation, impart some structure on the many variations and enhancements that have been proposed, and summarize the results from the key game and nongame domains to which MCTS methods have been applied. A number of open research questions indicate that the field is ripe for future work.},
	number = {1},
	journal = {IEEE Transactions on Computational Intelligence and AI in Games},
	author = {Browne, C. B. and Powley, E. and Whitehouse, D. and Lucas, S. M. and Cowling, P. I. and Rohlfshagen, P. and Tavener, S. and Perez, D. and Samothrakis, S. and Colton, S.},
	month = mar,
	year = {2012},
	keywords = {Artificial intelligence, Artificial intelligence (AI), bandit-based methods, computer Go, Computers, Decision theory, game search, game theory, Game theory, Games, key game, Markov processes, MCTS research, Monte Carlo methods, Monte Carlo tree search (MCTS), Monte carlo tree search methods, nongame domains, random sampling generality, tree searching, upper confidence bounds (UCB), upper confidence bounds for trees (UCT)},
	pages = {1--43}
}


@article{deisenroth_pilco:_nodate,
	title = {{PILCO}: {A} {Model}-{Based} and {Data}-{Efficient} {Approach} to {Policy} {Search}},
	abstract = {In this paper, we introduce pilco, a practical, data-eﬃcient model-based policy search method. Pilco reduces model bias, one of the key problems of model-based reinforcement learning, in a principled way. By learning a probabilistic dynamics model and explicitly incorporating model uncertainty into long-term planning, pilco can cope with very little data and facilitates learning from scratch in only a few trials. Policy evaluation is performed in closed form using state-ofthe-art approximate inference. Furthermore, policy gradients are computed analytically for policy improvement. We report unprecedented learning eﬃciency on challenging and high-dimensional control tasks.},
	language = {en},
	author = {Deisenroth, Marc Peter and Rasmussen, Carl Edward},
	pages = {8}
}


@article{li_recurrent_2015,
	title = {Recurrent {Reinforcement} {Learning}: {A} {Hybrid} {Approach}},
	shorttitle = {Recurrent {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1509.03044},
	abstract = {Successful applications of reinforcement learning in real-world problems often require dealing with partially observable states. It is in general very challenging to construct and infer hidden states as they often depend on the agent's entire interaction history and may require substantial domain knowledge. In this work, we investigate a deep-learning approach to learning the representation of states in partially observable tasks, with minimal prior knowledge of the domain. In particular, we propose a new family of hybrid models that combines the strength of both supervised learning (SL) and reinforcement learning (RL), trained in a joint fashion: The SL component can be a recurrent neural networks (RNN) or its long short-term memory (LSTM) version, which is equipped with the desired property of being able to capture long-term dependency on history, thus providing an effective way of learning the representation of hidden states. The RL component is a deep Q-network (DQN) that learns to optimize the control for maximizing long-term rewards. Extensive experiments in a direct mailing campaign problem demonstrate the effectiveness and advantages of the proposed approach, which performs the best among a set of previous state-of-the-art methods.},
	urldate = {2019-02-24},
	journal = {arXiv:1509.03044 [cs]},
	author = {Li, Xiujun and Li, Lihong and Gao, Jianfeng and He, Xiaodong and Chen, Jianshu and Deng, Li and He, Ji},
	month = sep,
	year = {2015},
	note = {arXiv: 1509.03044},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Systems and Control},
	annote = {Comment: 11 pages, 6 figures}
}


@article{zhang_dissection_2018,
	title = {A {Dissection} of {Overfitting} and {Generalization} in {Continuous} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1806.07937},
	abstract = {The risks and perils of overfitting in machine learning are well known. However most of the treatment of this, including diagnostic tools and remedies, was developed for the supervised learning case. In this work, we aim to offer new perspectives on the characterization and prevention of overfitting in deep Reinforcement Learning (RL) methods, with a particular focus on continuous domains. We examine several aspects, such as how to define and diagnose overfitting in MDPs, and how to reduce risks by injecting sufficient training diversity. This work complements recent findings on the brittleness of deep RL methods and offers practical observations for RL researchers and practitioners.},
	urldate = {2019-02-24},
	journal = {arXiv:1806.07937 [cs, stat]},
	author = {Zhang, Amy and Ballas, Nicolas and Pineau, Joelle},
	month = jun,
	year = {2018},
	note = {arXiv: 1806.07937},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: 20 pages, 16 figures},
	file = {arXiv\:1806.07937 PDF:C\:\\Users\\sumee\\Zotero\\storage\\9IBZL6GK\\Zhang et al. - 2018 - A Dissection of Overfitting and Generalization in .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\sumee\\Zotero\\storage\\S99248HN\\1806.html:text/html}
}

@article{zhang_study_2018,
	title = {A {Study} on {Overfitting} in {Deep} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1804.06893},
	abstract = {Recent years have witnessed significant progresses in deep Reinforcement Learning (RL). Empowered with large scale neural networks, carefully designed architectures, novel training algorithms and massively parallel computing devices, researchers are able to attack many challenging RL problems. However, in machine learning, more training power comes with a potential risk of more overfitting. As deep RL techniques are being applied to critical problems such as healthcare and finance, it is important to understand the generalization behaviors of the trained agents. In this paper, we conduct a systematic study of standard RL agents and find that they could overfit in various ways. Moreover, overfitting could happen "robustly": commonly used techniques in RL that add stochasticity do not necessarily prevent or detect overfitting. In particular, the same agents and learning algorithms could have drastically different test performance, even when all of them achieve optimal rewards during training. The observations call for more principled and careful evaluation protocols in RL. We conclude with a general discussion on overfitting in RL and a study of the generalization behaviors from the perspective of inductive bias.},
	urldate = {2019-02-24},
	journal = {arXiv:1804.06893 [cs, stat]},
	author = {Zhang, Chiyuan and Vinyals, Oriol and Munos, Remi and Bengio, Samy},
	month = apr,
	year = {2018},
	note = {arXiv: 1804.06893},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv\:1804.06893 PDF:C\:\\Users\\sumee\\Zotero\\storage\\L3B87RLP\\Zhang et al. - 2018 - A Study on Overfitting in Deep Reinforcement Learn.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\sumee\\Zotero\\storage\\M4RAB9CI\\1804.html:text/html}
}


@article{lowe_multi-agent_2017,
	title = {Multi-{Agent} {Actor}-{Critic} for {Mixed} {Cooperative}-{Competitive} {Environments}},
	url = {http://arxiv.org/abs/1706.02275},
	abstract = {We explore deep reinforcement learning methods for multi-agent domains. We begin by analyzing the difficulty of traditional algorithms in the multi-agent case: Q-learning is challenged by an inherent non-stationarity of the environment, while policy gradient suffers from a variance that increases as the number of agents grows. We then present an adaptation of actor-critic methods that considers action policies of other agents and is able to successfully learn policies that require complex multi-agent coordination. Additionally, we introduce a training regimen utilizing an ensemble of policies for each agent that leads to more robust multi-agent policies. We show the strength of our approach compared to existing methods in cooperative as well as competitive scenarios, where agent populations are able to discover various physical and informational coordination strategies.},
	urldate = {2019-02-24},
	journal = {arXiv:1706.02275 [cs]},
	author = {Lowe, Ryan and Wu, Yi and Tamar, Aviv and Harb, Jean and Abbeel, Pieter and Mordatch, Igor},
	month = jun,
	year = {2017},
	note = {arXiv: 1706.02275},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing}
}

@article{peng_multiagent_2017,
	title = {Multiagent {Bidirectionally}-{Coordinated} {Nets}: {Emergence} of {Human}-level {Coordination} in {Learning} to {Play} {StarCraft} {Combat} {Games}},
	shorttitle = {Multiagent {Bidirectionally}-{Coordinated} {Nets}},
	url = {http://arxiv.org/abs/1703.10069},
	abstract = {Many artificial intelligence (AI) applications often require multiple intelligent agents to work in a collaborative effort. Efficient learning for intra-agent communication and coordination is an indispensable step towards general AI. In this paper, we take StarCraft combat game as a case study, where the task is to coordinate multiple agents as a team to defeat their enemies. To maintain a scalable yet effective communication protocol, we introduce a Multiagent Bidirectionally-Coordinated Network (BiCNet ['bIknet]) with a vectorised extension of actor-critic formulation. We show that BiCNet can handle different types of combats with arbitrary numbers of AI agents for both sides. Our analysis demonstrates that without any supervisions such as human demonstrations or labelled data, BiCNet could learn various types of advanced coordination strategies that have been commonly used by experienced game players. In our experiments, we evaluate our approach against multiple baselines under different scenarios; it shows state-of-the-art performance, and possesses potential values for large-scale real-world applications.},
	urldate = {2019-02-24},
	journal = {arXiv:1703.10069 [cs]},
	author = {Peng, Peng and Wen, Ying and Yang, Yaodong and Yuan, Quan and Tang, Zhenkun and Long, Haitao and Wang, Jun},
	month = mar,
	year = {2017},
	note = {arXiv: 1703.10069},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	annote = {Comment: 10 pages, 10 figures. Previously as title: "Multiagent Bidirectionally-Coordinated Nets for Learning to Play StarCraft Combat Games", Mar 2017}
}

@article{yang_mean_2018,
	title = {Mean {Field} {Multi}-{Agent} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1802.05438},
	abstract = {Existing multi-agent reinforcement learning methods are limited typically to a small number of agents. When the agent number increases largely, the learning becomes intractable due to the curse of the dimensionality and the exponential growth of agent interactions. In this paper, we present Mean Field Reinforcement Learning where the interactions within the population of agents are approximated by those between a single agent and the average effect from the overall population or neighboring agents; the interplay between the two entities is mutually reinforced: the learning of the individual agent's optimal policy depends on the dynamics of the population, while the dynamics of the population change according to the collective patterns of the individual policies. We develop practical mean field Q-learning and mean field Actor-Critic algorithms and analyze the convergence of the solution to Nash equilibrium. Experiments on Gaussian squeeze, Ising model, and battle games justify the learning effectiveness of our mean field approaches. In addition, we report the first result to solve the Ising model via model-free reinforcement learning methods.},
	urldate = {2019-02-24},
	journal = {arXiv:1802.05438 [cs]},
	author = {Yang, Yaodong and Luo, Rui and Li, Minne and Zhou, Ming and Zhang, Weinan and Wang, Jun},
	month = feb,
	year = {2018},
	note = {arXiv: 1802.05438},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Multiagent Systems}
}

@article{bansal_emergent_2017,
	title = {Emergent {Complexity} via {Multi}-{Agent} {Competition}},
	url = {http://arxiv.org/abs/1710.03748},
	abstract = {Reinforcement learning algorithms can train agents that solve problems in complex, interesting environments. Normally, the complexity of the trained agent is closely related to the complexity of the environment. This suggests that a highly capable agent requires a complex environment for training. In this paper, we point out that a competitive multi-agent environment trained with self-play can produce behaviors that are far more complex than the environment itself. We also point out that such environments come with a natural curriculum, because for any skill level, an environment full of agents of this level will have the right level of difﬁculty.},
	language = {en},
	urldate = {2019-02-14},
	journal = {arXiv:1710.03748 [cs]},
	author = {Bansal, Trapit and Pachocki, Jakub and Sidor, Szymon and Sutskever, Ilya and Mordatch, Igor},
	month = oct,
	year = {2017},
	note = {arXiv: 1710.03748},
	keywords = {Computer Science - Artificial Intelligence},
	annote = {Comment: Published as a conference paper at ICLR 2018},
	file = {Bansal et al. - 2017 - Emergent Complexity via Multi-Agent Competition.pdf:C\:\\Users\\sumee\\Zotero\\storage\\KKNRBZ9M\\Bansal et al. - 2017 - Emergent Complexity via Multi-Agent Competition.pdf:application/pdf}
}


@article{conitzer_awesome:_2007,
	title = {{AWESOME}: {A} general multiagent learning algorithm that converges in self-play and learns a best response against stationary opponents},
	volume = {67},
	issn = {0885-6125, 1573-0565},
	shorttitle = {{AWESOME}},
	url = {http://link.springer.com/10.1007/s10994-006-0143-1},
	doi = {10.1007/s10994-006-0143-1},
	abstract = {Two minimal requirements for a satisfactory multiagent learning algorithm are that it 1. learns to play optimally against stationary opponents and 2. converges to a Nash equilibrium in self-play. The previous algorithm that has come closest, WoLF-IGA, has been proven to have these two properties in 2-player 2-action (repeated) games—assuming that the opponent’s mixed strategy is observable. Another algorithm, ReDVaLeR (which was introduced after the algorithm described in this paper), achieves the two properties in games with arbitrary numbers of actions and players, but still requires that the opponents’ mixed strategies are observable. In this paper we present AWESOME, the ﬁrst algorithm that is guaranteed to have the two properties in games with arbitrary numbers of actions and players. It is still the only algorithm that does so while only relying on observing the other players’ actual actions (not their mixed strategies). It also learns to play optimally against opponents that eventually become stationary. The basic idea behind AWESOME (Adapt When Everybody is Stationary, Otherwise Move to Equilibrium) is to try to adapt to the others’ strategies when they appear stationary, but otherwise to retreat to a precomputed equilibrium strategy. We provide experimental results that suggest that AWESOME converges fast in practice. The techniques used to prove the properties of AWESOME are fundamentally diﬀerent from those used for previous algorithms, and may help in analyzing future multiagent learning algorithms as well.},
	language = {en},
	number = {1-2},
	urldate = {2019-02-24},
	journal = {Machine Learning},
	author = {Conitzer, Vincent and Sandholm, Tuomas},
	month = may,
	year = {2007},
	pages = {23--43}
}

@article{tesauro_extending_nodate,
	title = {Extending {Q}-{Learning} to {General} {Adaptive} {Multi}-{Agent} {Systems}},
	abstract = {Recent multi-agent extensions of Q-Learning require knowledge of other agents’ payoffs and Q-functions, and assume game-theoretic play at all times by all other agents. This paper proposes a fundamentally different approach, dubbed “Hyper-Q” Learning, in which values of mixed strategies rather than base actions are learned, and in which other agents’ strategies are estimated from observed actions via Bayesian inference. Hyper-Q may be effective against many different types of adaptive agents, even if they are persistently dynamic. Against certain broad categories of adaptation, it is argued that Hyper-Q may converge to exact optimal time-varying policies. In tests using Rock-Paper-Scissors, Hyper-Q learns to significantly exploit an Infinitesimal Gradient Ascent (IGA) player, as well as a Policy Hill Climber (PHC) player. Preliminary analysis of Hyper-Q against itself is also presented.},
	language = {en},
	author = {Tesauro, Gerald},
	pages = {8}
}


@article{foerster_learning_2017,
	title = {Learning with {Opponent}-{Learning} {Awareness}},
	url = {http://arxiv.org/abs/1709.04326},
	abstract = {Multi-agent settings are quickly gathering importance in machine learning. This includes a plethora of recent work on deep multi-agent reinforcement learning, but also can be extended to hierarchical RL, generative adversarial networks and decentralised optimisation. In all these settings the presence of multiple learning agents renders the training problem non-stationary and often leads to unstable training or undesired final results. We present Learning with Opponent-Learning Awareness (LOLA), a method in which each agent shapes the anticipated learning of the other agents in the environment. The LOLA learning rule includes a term that accounts for the impact of one agent's policy on the anticipated parameter update of the other agents. Results show that the encounter of two LOLA agents leads to the emergence of tit-for-tat and therefore cooperation in the iterated prisoners' dilemma, while independent learning does not. In this domain, LOLA also receives higher payouts compared to a naive learner, and is robust against exploitation by higher order gradient-based methods. Applied to repeated matching pennies, LOLA agents converge to the Nash equilibrium. In a round robin tournament we show that LOLA agents successfully shape the learning of a range of multi-agent learning algorithms from literature, resulting in the highest average returns on the IPD. We also show that the LOLA update rule can be efficiently calculated using an extension of the policy gradient estimator, making the method suitable for model-free RL. The method thus scales to large parameter and input spaces and nonlinear function approximators. We apply LOLA to a grid world task with an embedded social dilemma using recurrent policies and opponent modelling. By explicitly considering the learning of the other agent, LOLA agents learn to cooperate out of self-interest. The code is at github.com/alshedivat/lola.},
	urldate = {2019-02-24},
	journal = {arXiv:1709.04326 [cs]},
	author = {Foerster, Jakob N. and Chen, Richard Y. and Al-Shedivat, Maruan and Whiteson, Shimon and Abbeel, Pieter and Mordatch, Igor},
	month = sep,
	year = {2017},
	note = {arXiv: 1709.04326},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Science and Game Theory}
}


@article{le_coordinated_2017,
	title = {Coordinated {Multi}-{Agent} {Imitation} {Learning}},
	url = {http://arxiv.org/abs/1703.03121},
	abstract = {We study the problem of imitation learning from demonstrations of multiple coordinating agents. One key challenge in this setting is that learning a good model of coordination can be difficult, since coordination is often implicit in the demonstrations and must be inferred as a latent variable. We propose a joint approach that simultaneously learns a latent coordination model along with the individual policies. In particular, our method integrates unsupervised structure learning with conventional imitation learning. We illustrate the power of our approach on a difficult problem of learning multiple policies for fine-grained behavior modeling in team sports, where different players occupy different roles in the coordinated team strategy. We show that having a coordination model to infer the roles of players yields substantially improved imitation loss compared to conventional baselines.},
	urldate = {2019-02-24},
	journal = {arXiv:1703.03121 [cs]},
	author = {Le, Hoang M. and Yue, Yisong and Carr, Peter and Lucey, Patrick},
	month = mar,
	year = {2017},
	note = {arXiv: 1703.03121},
	keywords = {Computer Science - Machine Learning},
	annote = {Comment: International Conference on Machine Learning 2017},
	file = {arXiv\:1703.03121 PDF:C\:\\Users\\sumee\\Zotero\\storage\\CQHP2F72\\Le et al. - 2017 - Coordinated Multi-Agent Imitation Learning.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\sumee\\Zotero\\storage\\SBH99EHT\\1703.html:text/html}
}

@article{zhang_coordinating_nodate,
	title = {Coordinating {Multi}-{Agent} {Reinforcement} {Learning} with {Limited} {Communication}},
	abstract = {Coordinated multi-agent reinforcement learning (MARL) provides a promising approach to scaling learning in large cooperative multiagent systems. Distributed constraint optimization (DCOP) techniques have been used to coordinate action selection among agents during both the learning phase and the policy execution phase (if learning is off-line) to ensure good overall system performance. However, running DCOP algorithms for each action selection through the whole system results in signiﬁcant communication among agents, which is not practical for most applications with limited communication bandwidth. In this paper, we develop a learning approach that generalizes previous coordinated MARL approaches that use DCOP algorithms and enables MARL to be conducted over a spectrum from independent learning (without communication) to fully coordinated learning depending on agents’ communication bandwidth. Our approach deﬁnes an interaction measure that allows agents to dynamically identify their beneﬁcial coordination set (i.e., whom to coordinate with) in different situations and to trade off its performance and communication cost. By limiting their coordination set, agents dynamically decompose the coordination network in a distributed way, resulting in dramatically reduced communication for DCOP algorithms without signiﬁcantly affecting overall learning performance. Essentially, our learning approach conducts co-adaptation of agents’ policy learning and coordination set identiﬁcation, which outperforms approaches that sequence them.},
	language = {en},
	author = {Zhang, Chongjie and Lesser, Victor},
	pages = {8}
}

@inproceedings{Rusch2018ExplorationExploitationTI,
  title={Exploration-Exploitation Trade-off in Deep Reinforcement Learning},
  author={Leslie A. Rusch},
  year={2018}
}

@article{inverseRL,
  author    = {Xiaomin Lin and
               Peter A. Beling and
               Randy Cogill},
  title     = {Multi-agent Inverse Reinforcement Learning for Zero-sum Games},
  journal   = {CoRR},
  volume    = {abs/1403.6508},
  year      = {2014},
  url       = {http://arxiv.org/abs/1403.6508},
  archivePrefix = {arXiv},
  eprint    = {1403.6508},
  timestamp = {Mon, 13 Aug 2018 16:46:32 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/LinBC14},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{PPO,
  author    = {John Schulman and
               Filip Wolski and
               Prafulla Dhariwal and
               Alec Radford and
               Oleg Klimov},
  title     = {Proximal Policy Optimization Algorithms},
  journal   = {CoRR},
  volume    = {abs/1707.06347},
  year      = {2017},
  url       = {http://arxiv.org/abs/1707.06347},
  archivePrefix = {arXiv},
  eprint    = {1707.06347},
  timestamp = {Mon, 13 Aug 2018 16:47:34 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/SchulmanWDRK17},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{TRPO,
  author    = {John Schulman and
               Sergey Levine and
               Philipp Moritz and
               Michael I. Jordan and
               Pieter Abbeel},
  title     = {Trust Region Policy Optimization},
  journal   = {CoRR},
  volume    = {abs/1502.05477},
  year      = {2015},
  url       = {http://arxiv.org/abs/1502.05477},
  archivePrefix = {arXiv},
  eprint    = {1502.05477},
  timestamp = {Mon, 13 Aug 2018 16:48:08 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/SchulmanLMJA15},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{TRPPO,
  author    = {Yuhui Wang and
               Hao He and
               Xiaoyang Tan and
               Yaozhong Gan},
  title     = {Trust Region-Guided Proximal Policy Optimization},
  journal   = {CoRR},
  volume    = {abs/1901.10314},
  year      = {2019},
  url       = {http://arxiv.org/abs/1901.10314},
  archivePrefix = {arXiv},
  eprint    = {1901.10314},
  timestamp = {Sat, 02 Feb 2019 16:56:00 +0100},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1901-10314},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{klima_markov_nodate,
	title = {Markov {Security} {Games}: {Learning} in {Spatial} {Security} {Problems}},
	abstract = {In this paper we present a preliminary investigation of modelling spatial aspects of security games within the context of Markov games. Reinforcement learning is a powerful tool for adaptation in unknown environments, however the basic singleagent RL algorithms are unﬁt to be applied in adversarial scenarios. Therefore, we proﬁt from Adversarial Multi-Armed Bandit (AMAB) methods which are designed for such situations. Based on temporal difference methods we derive two new multiagent algorithms using AMAB methods for spatial two-player non-cooperative security games.},
	language = {en},
	author = {Klima, Richard and Tuyls, Karl and Oliehoek, Frans},
	pages = {8},
	file = {Klima et al. - Markov Security Games Learning in Spatial Securit.pdf:C\:\\Users\\sumee\\Zotero\\storage\\WNDDC7U3\\Klima et al. - Markov Security Games Learning in Spatial Securit.pdf:application/pdf}
}

@inproceedings{yasuyuki_cooperative_2015,
	title = {Cooperative capture by multi-agent using reinforcement learning application for security patrol systems},
	doi = {10.1109/ASCC.2015.7244682},
	abstract = {Aim of this study is to create a security patrol system which is a cooperative capturing system by using multi-agent in the building. A host computer deploys autonomous robots as agents and find the best strategy to enclose an intruder. From the view point of the pursuit problem by multi-agent, reinforcement learning theory is one of choices to find way how to enclose an intruder. In order to apply reinforcement learning theory to security patrol systems, this study introduces how to discretize patrol areas. Some RFID tags are embedded in the floor and each autonomous robot can know the location where it is by sensing RFID tags, then sends locational information to the host computer. The host computer calculates positioning of the autonomous robots based on received locational data through wireless network. We make a prototype of patrol system and show how it works in this paper.},
	booktitle = {2015 10th {Asian} {Control} {Conference} ({ASCC})},
	author = {Yasuyuki, S. and Hirofumi, O. and Tadashi, M. and Maya, H.},
	month = may,
	year = {2015},
	keywords = {autonomous robots, Computers, cooperative capturing system, host computer, learning (artificial intelligence), Learning (artificial intelligence), mobile robots, Mobile robots, multi-agent systems, multiagent system, Radiofrequency identification, received locational data, reinforcement learning theory, RFID tags, Robot kinematics, security, Security, security patrol systems, wireless network},
	pages = {1--6},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\sumee\\Zotero\\storage\\QPXGYKWF\\7244682.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\sumee\\Zotero\\storage\\V85JYVHG\\Yasuyuki et al. - 2015 - Cooperative capture by multi-agent using reinforce.pdf:application/pdf}
}


@article{heinrich_deep_2016,
	title = {Deep {Reinforcement} {Learning} from {Self}-{Play} in {Imperfect}-{Information} {Games}},
	url = {http://arxiv.org/abs/1603.01121},
	abstract = {Many real-world applications can be described as large-scale games of imperfect information. To deal with these challenging domains, prior work has focused on computing Nash equilibria in a handcrafted abstraction of the domain. In this paper we introduce the ﬁrst scalable end-to-end approach to learning approximate Nash equilibria without prior domain knowledge. Our method combines ﬁctitious self-play with deep reinforcement learning. When applied to Leduc poker, Neural Fictitious Self-Play (NFSP) approached a Nash equilibrium, whereas common reinforcement learning methods diverged. In Limit Texas Hold’em, a poker game of real-world scale, NFSP learnt a strategy that approached the performance of state-of-the-art, superhuman algorithms based on signiﬁcant domain expertise.},
	language = {en},
	urldate = {2019-02-24},
	journal = {arXiv:1603.01121 [cs]},
	author = {Heinrich, Johannes and Silver, David},
	month = mar,
	year = {2016},
	note = {arXiv: 1603.01121},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Science and Game Theory, Computer Science - Machine Learning},
	annote = {Comment: updated version, incorporating conference feedback},
	file = {Heinrich and Silver - 2016 - Deep Reinforcement Learning from Self-Play in Impe.pdf:C\:\\Users\\sumee\\Zotero\\storage\\LJSRLSHS\\Heinrich and Silver - 2016 - Deep Reinforcement Learning from Self-Play in Impe.pdf:application/pdf}
}

@article{lanctot_unified_2017,
	title = {A {Unified} {Game}-{Theoretic} {Approach} to {Multiagent} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1711.00832},
	abstract = {To achieve general intelligence, agents must learn how to interact with others in a shared environment: this is the challenge of multiagent reinforcement learning (MARL). The simplest form is independent reinforcement learning (InRL), where each agent treats its experience as part of its (non-stationary) environment. In this paper, we ﬁrst observe that policies learned using InRL can overﬁt to the other agents’ policies during training, failing to sufﬁciently generalize during execution. We introduce a new metric, joint-policy correlation, to quantify this effect. We describe an algorithm for general MARL, based on approximate best responses to mixtures of policies generated using deep reinforcement learning, and empirical game-theoretic analysis to compute meta-strategies for policy selection. The algorithm generalizes previous ones such as InRL, iterated best response, double oracle, and ﬁctitious play. Then, we present a scalable implementation which reduces the memory requirement using decoupled meta-solvers. Finally, we demonstrate the generality of the resulting policies in two partially observable settings: gridworld coordination games and poker.},
	language = {en},
	urldate = {2019-02-24},
	journal = {arXiv:1711.00832 [cs]},
	author = {Lanctot, Marc and Zambaldi, Vinicius and Gruslys, Audrunas and Lazaridou, Angeliki and Tuyls, Karl and Perolat, Julien and Silver, David and Graepel, Thore},
	month = nov,
	year = {2017},
	note = {arXiv: 1711.00832},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Science and Game Theory, Computer Science - Machine Learning, Computer Science - Multiagent Systems},
	annote = {Comment: Camera-ready copy of NIPS 2017 paper, including appendix}
}

@incollection{brown_fp1951,
	Address = {New York},
	Author = {George W. Brown},
	Booktitle = {Activity Analysis of Production and Allocation},
	Editor = {T. C. Koopmans},
	Publisher = {Wiley},
	Title = {Iterative Solution of Games by Fictitious Play},
	Year = {1951}}

@article{heinrich_fictitious_nodate,
	title = {Fictitious {Self}-{Play} in {Extensive}-{Form} {Games}},
	abstract = {Fictitious play is a popular game-theoretic model of learning in games. However, it has received little attention in practical applications to large problems. This paper introduces two variants of ﬁctitious play that are implemented in behavioural strategies of an extensive-form game. The ﬁrst variant is a full-width process that is realization equivalent to its normal-form counterpart and therefore inherits its convergence guarantees. However, its computational requirements are linear in time and space rather than exponential. The second variant, Fictitious Self-Play, is a machine learning framework that implements ﬁctitious play in a sample-based fashion. Experiments in imperfect-information poker games compare our approaches and demonstrate their convergence to approximate Nash equilibria.},
	language = {en},
	author = {Heinrich, Johannes and Lanctot, Marc},
	pages = {9},
	year ={2015}
}

@article{kapoor_multi-agent_2018,
	title = {Multi-{Agent} {Reinforcement} {Learning}: {A} {Report} on {Challenges} and {Approaches}},
	shorttitle = {Multi-{Agent} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1807.09427},
	abstract = {Reinforcement Learning (RL) is a learning paradigm concerned with learning to control a system so as to maximize an objective over the long term. This approach to learning has received immense interest in recent times and success manifests itself in the form of human-level performance on games like Go. While RL is emerging as a practical component in reallife systems, most successes have been in Single Agent domains. This report will instead speciﬁcally focus on challenges that are unique to Multi-Agent Systems interacting in mixed cooperative and competitive environments. The report concludes with advances in the paradigm of training Multi-Agent Systems called Decentralized Actor, Centralized Critic, based on an extension of MDPs called Decentralized Partially Observable MDPs, which has seen a renewed interest lately.},
	language = {en},
	urldate = {2019-02-14},
	journal = {arXiv:1807.09427 [cs, stat]},
	author = {Kapoor, Sanyam},
	month = jul,
	year = {2018},
	note = {arXiv: 1807.09427},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: 25 pages, 6 figures}
}

@article{browne_survey_2012,
	title = {A {Survey} of {Monte} {Carlo} {Tree} {Search} {Methods}},
	volume = {4},
	issn = {1943-068X},
	doi = {10.1109/TCIAIG.2012.2186810},
	abstract = {Monte Carlo tree search (MCTS) is a recently proposed search method that combines the precision of tree search with the generality of random sampling. It has received considerable interest due to its spectacular success in the difficult problem of computer Go, but has also proved beneficial in a range of other domains. This paper is a survey of the literature to date, intended to provide a snapshot of the state of the art after the first five years of MCTS research. We outline the core algorithm's derivation, impart some structure on the many variations and enhancements that have been proposed, and summarize the results from the key game and nongame domains to which MCTS methods have been applied. A number of open research questions indicate that the field is ripe for future work.},
	number = {1},
	journal = {IEEE Transactions on Computational Intelligence and AI in Games},
	author = {Browne, C. B. and Powley, E. and Whitehouse, D. and Lucas, S. M. and Cowling, P. I. and Rohlfshagen, P. and Tavener, S. and Perez, D. and Samothrakis, S. and Colton, S.},
	month = mar,
	year = {2012},
	keywords = {Computers, Artificial intelligence, Artificial intelligence (AI), bandit-based methods, computer Go, Decision theory, game search, game theory, Game theory, Games, key game, Markov processes, MCTS research, Monte Carlo methods, Monte Carlo tree search (MCTS), Monte carlo tree search methods, nongame domains, random sampling generality, tree searching, upper confidence bounds (UCB), upper confidence bounds for trees (UCT)},
	pages = {1--43}
}


@article{schmitt_kickstarting_2018,
	title = {Kickstarting {Deep} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1803.03835},
	abstract = {We present a method for using previously-trained 'teacher' agents to kickstart the training of a new 'student' agent. To this end, we leverage ideas from policy distillation and population based training. Our method places no constraints on the architecture of the teacher or student agents, and it regulates itself to allow the students to surpass their teachers in performance. We show that, on a challenging and computationally-intensive multi-task benchmark (DMLab-30), kickstarted training improves the data efficiency of new agents, making it significantly easier to iterate on their design. We also show that the same kickstarting pipeline can allow a single student agent to leverage multiple 'expert' teachers which specialize on individual tasks. In this setting kickstarting yields surprisingly large gains, with the kickstarted agent matching the performance of an agent trained from scratch in almost 10x fewer steps, and surpassing its final performance by 42 percent. Kickstarting is conceptually simple and can easily be incorporated into reinforcement learning experiments.},
	urldate = {2019-02-27},
	journal = {arXiv:1803.03835 [cs]},
	author = {Schmitt, Simon and Hudson, Jonathan J. and Zidek, Augustin and Osindero, Simon and Doersch, Carl and Czarnecki, Wojciech M. and Leibo, Joel Z. and Kuttler, Heinrich and Zisserman, Andrew and Simonyan, Karen and Eslami, S. M. Ali},
	month = mar,
	year = {2018},
	note = {arXiv: 1803.03835},
	keywords = {Computer Science - Machine Learning}
}


@article{silver_mastering_chess_2017,
	title = {Mastering {Chess} and {Shogi} by {Self}-{Play} with a {General} {Reinforcement} {Learning} {Algorithm}},
	url = {https://arxiv.org/abs/1712.01815v1},
	abstract = {The game of chess is the most widely-studied domain in the history of
artificial intelligence. The strongest programs are based on a combination of
sophisticated search techniques, domain-specific adaptations, and handcrafted
evaluation functions that have been refined by human experts over several
decades. In contrast, the AlphaGo Zero program recently achieved superhuman
performance in the game of Go, by tabula rasa reinforcement learning from games
of self-play. In this paper, we generalise this approach into a single
AlphaZero algorithm that can achieve, tabula rasa, superhuman performance in
many challenging domains. Starting from random play, and given no domain
knowledge except the game rules, AlphaZero achieved within 24 hours a
superhuman level of play in the games of chess and shogi (Japanese chess) as
well as Go, and convincingly defeated a world-champion program in each case.},
	language = {en},
	urldate = {2019-02-28},
	author = {Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and Lillicrap, Timothy and Simonyan, Karen and Hassabis, Demis},
	month = dec,
	year = {2017}
}


@article{charlesworth_application_2018,
	title = {Application of {Self}-{Play} {Reinforcement} {Learning} to a {Four}-{Player} {Game} of {Imperfect} {Information}},
	url = {https://arxiv.org/abs/1808.10442v1},
	abstract = {We introduce a new virtual environment for simulating a card game known as
"Big 2". This is a four-player game of imperfect information with a relatively
complicated action space (being allowed to play 1,2,3,4 or 5 card combinations
from an initial starting hand of 13 cards). As such it poses a challenge for
many current reinforcement learning methods. We then use the recently proposed
"Proximal Policy Optimization" algorithm to train a deep neural network to play
the game, purely learning via self-play, and find that it is able to reach a
level which outperforms amateur human players after only a relatively short
amount of training time and without needing to search a tree of future game
states.},
	language = {en},
	urldate = {2019-02-28},
	author = {Charlesworth, Henry},
	month = aug,
	year = {2018}
}



@article{heinrich_deep_2016,
	title = {Deep {Reinforcement} {Learning} from {Self}-{Play} in {Imperfect}-{Information} {Games}},
	url = {http://arxiv.org/abs/1603.01121},
	abstract = {Many real-world applications can be described as large-scale games of imperfect information. To deal with these challenging domains, prior work has focused on computing Nash equilibria in a handcrafted abstraction of the domain. In this paper we introduce the ﬁrst scalable end-to-end approach to learning approximate Nash equilibria without prior domain knowledge. Our method combines ﬁctitious self-play with deep reinforcement learning. When applied to Leduc poker, Neural Fictitious Self-Play (NFSP) approached a Nash equilibrium, whereas common reinforcement learning methods diverged. In Limit Texas Hold’em, a poker game of real-world scale, NFSP learnt a strategy that approached the performance of state-of-the-art, superhuman algorithms based on signiﬁcant domain expertise.},
	language = {en},
	urldate = {2019-02-24},
	journal = {arXiv:1603.01121 [cs]},
	author = {Heinrich, Johannes and Silver, David},
	month = mar,
	year = {2016},
	note = {arXiv: 1603.01121},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Science and Game Theory, Computer Science - Machine Learning},
	annote = {Comment: updated version, incorporating conference feedback}
}

@article{heinrich_fictitious_nodate,
	title = {Fictitious {Self}-{Play} in {Extensive}-{Form} {Games}},
	abstract = {Fictitious play is a popular game-theoretic model of learning in games. However, it has received little attention in practical applications to large problems. This paper introduces two variants of ﬁctitious play that are implemented in behavioural strategies of an extensive-form game. The ﬁrst variant is a full-width process that is realization equivalent to its normal-form counterpart and therefore inherits its convergence guarantees. However, its computational requirements are linear in time and space rather than exponential. The second variant, Fictitious Self-Play, is a machine learning framework that implements ﬁctitious play in a sample-based fashion. Experiments in imperfect-information poker games compare our approaches and demonstrate their convergence to approximate Nash equilibria.},
	language = {en},
	author = {Heinrich, Johannes and Lanctot, Marc},
	pages = {9}
}

@article{kawamura_neural_2019,
	title = {Neural {Fictitious} {Self}-{Play} on {ELF} {Mini}-{RTS}},
	url = {https://arxiv.org/abs/1902.02004v1},
	abstract = {Despite the notable successes in video games such as Atari 2600, current AI
is yet to defeat human champions in the domain of real-time strategy (RTS)
games. One of the reasons is that an RTS game is a multi-agent game, in which
single-agent reinforcement learning methods cannot simply be applied because
the environment is not a stationary Markov Decision Process. In this paper, we
present a first step toward finding a game-theoretic solution to RTS games by
applying Neural Fictitious Self-Play (NFSP), a game-theoretic approach for
finding Nash equilibria, to Mini-RTS, a small but nontrivial RTS game provided
on the ELF platform. More specifically, we show that NFSP can be effectively
combined with policy gradient reinforcement learning and be applied to
Mini-RTS. Experimental results also show that the scalability of NFSP can be
substantially improved by pretraining the models with simple self-play using
policy gradients, which by itself gives a strong strategy despite its lack of
theoretical guarantee of convergence.},
	language = {en},
	urldate = {2019-02-28},
	author = {Kawamura, Keigo and Tsuruoka, Yoshimasa},
	month = feb,
	year = {2019}
}
